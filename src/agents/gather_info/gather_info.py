"""
ì •ë³´_ìˆ˜ì§‘ ê·¸ë˜í”„ êµ¬ì„± ë° ì»´íŒŒì¼
ë¬¸ì„œ gather_info.mdì— ëª…ì‹œëœ ê·¸ë˜í”„ êµ¬ì¡° êµ¬í˜„

ê·¸ë˜í”„ íë¦„:
1. load_user_profile: ì‚¬ìš©ì í”„ë¡œí•„ ë¡œë“œ
2. information_gathering: ì •ë³´ ìˆ˜ì§‘ ë° LLM ì²˜ë¦¬ (ë°˜ë³µ ê°€ëŠ¥)
3. ì¡°ê±´ë¶€ ë¼ìš°íŒ…: ì™„ë£Œ ì‹œ END, ê³„ì† ì‹œ ëŒ€ê¸°

í•µì‹¬ íŠ¹ì§•:
- ì ì§„ì  ìƒíƒœ ì—…ë°ì´íŠ¸ ì‚¬ì´í´ (ì½ê¸° â†’ ì²˜ë¦¬ â†’ ì“°ê¸°)
- ê°œì¸í™”ëœ ëŒ€í™” (ì‚¬ìš©ì í”„ë¡œí•„ í™œìš©)
- ë³µí•© ì˜ë„ íŒŒì•… (ì œì™¸ ì¡°ê±´, ë³µí•© ìš”êµ¬ì‚¬í•­)
- ì ê·¹ì  ì œì•ˆ ê¸°ëŠ¥ (ë§‰ì—°í•œ ë‹µë³€ ì‹œ ì„ íƒì§€ ì œê³µ)
"""

import os
from typing import Optional
from langgraph.graph import StateGraph, END, START
from langgraph.checkpoint.memory import MemorySaver
import httpx
from agents.gather_info.state import ConversationState


from typing import Dict, Any
from langchain_google_genai import ChatGoogleGenerativeAI

from agents.gather_info.tools import SearchInfo
from agents.gather_info.state import ConversationState
from agents.gather_info.prompt import build_information_gathering_prompt, format_prompt_variables

def next_node(state: ConversationState) -> Dict[str, Any]:
    """
    ë‹¤ìŒ ë…¸ë“œë¡œ ì´ë™
    """
    print("ğŸ”„ ë‹¤ìŒ ë…¸ë“œë¡œ ì´ë™")
    return ConversationState(current_step="next_node")

def information_gathering_node(state: ConversationState) -> Dict[str, Any]:
    """
    ë¬¸ì„œì— ëª…ì‹œëœ 'ì½ê¸° â†’ ì²˜ë¦¬ â†’ ì“°ê¸°' ì‚¬ì´í´ êµ¬í˜„:
    - ì½ê¸°: í˜„ì¬ ìƒíƒœì—ì„œ ëŒ€í™” ë‚´ì—­, ê²€ìƒ‰ ì¡°ê±´, ì‚¬ìš©ì í”„ë¡œí•„ ì½ê¸°
    - ì²˜ë¦¬: LLMì´ ë„êµ¬ í˜¸ì¶œì„ í†µí•´ ë‹¤ìŒ í–‰ë™ ê²°ì • (ì§ˆë¬¸ or ê²€ìƒ‰)
    - ì“°ê¸°: ë„êµ¬ í˜¸ì¶œ ê²°ê³¼ì— ë”°ë¼ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    print("ğŸ§  ì •ë³´ ìˆ˜ì§‘ ë° ë¶„ì„ ì¤‘...")
    
    # 1ë‹¨ê³„: ì½ê¸° - í˜„ì¬ ìƒíƒœ ì½ê¸°
    current_messages = state.get("messages", [])
    current_criteria = state.get("search_criteria", SearchInfo())
    user_profile = state.get("user_profile")
    user_message = state.get("user_message", "")
    
    # 2ë‹¨ê³„: ì²˜ë¦¬ - LLMì—ê²Œ ìƒí™© ì „ë‹¬ ë° ë„êµ¬ í˜¸ì¶œì„ í†µí•œ ë‹¤ìŒ í–‰ë™ ê²°ì •
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite", temperature=0.3)
    llm_with_tools = llm.bind_tools([SearchInfo])
    
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„± ë° ë³€ìˆ˜ í¬ë§·íŒ…
    prompt = build_information_gathering_prompt()
    prompt_variables = format_prompt_variables(user_profile, current_criteria, user_message)
    print("prompt_variables: ", prompt_variables)
    # ì²´ì¸ êµ¬ì„±
    chain = prompt | llm_with_tools
    
    # LLM í˜¸ì¶œ
    ai_response = chain.invoke({
        "messages": current_messages,
        **prompt_variables
    })
    
    # 3ë‹¨ê³„: ì“°ê¸° - ë„êµ¬ í˜¸ì¶œ ê²°ê³¼ì— ë”°ë¼ ìƒíƒœ ì—…ë°ì´íŠ¸
    updated_state = _process_tool_call_response(ai_response, current_criteria)
    print("updated_state: ", updated_state)
    return ConversationState(**updated_state)



def _process_tool_call_response(ai_response, current_criteria: SearchInfo) -> Dict[str, Any]:
    """
    ë„êµ¬ í˜¸ì¶œ ì‘ë‹µì„ ì²˜ë¦¬í•˜ì—¬ ìƒíƒœ ì—…ë°ì´íŠ¸ ì •ë³´ ë°˜í™˜
    """

    if hasattr(ai_response, 'tool_calls') and ai_response.tool_calls:
        return {"search_info": ai_response.tool_calls[0]['args'] , "is_info_gathering_complete": True}
    else:
        return {"messages": [ai_response.content]}



def should_continue_gathering(state: ConversationState) -> str:
    """
    ì¡°ê±´ë¶€ ì—£ì§€: ë„êµ¬ í˜¸ì¶œ íƒ€ì…ì— ë”°ë¼ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •
    
    Returns:
        "complete": SearchInfo ë„êµ¬ í˜¸ì¶œë¨, ê²€ìƒ‰ ë‹¨ê³„ë¡œ ì´ë™
        "continue": AskQuestionAndUpdateState í˜¸ì¶œë¨, ê³„ì† ëŒ€í™” í•„ìš”
    """
    if state.get("is_info_gathering_complete"):
        return "complete"
    else:
        return "continue"



def build_graph(http_session: httpx.AsyncClient):
    """
    ì •ë³´_ìˆ˜ì§‘ ê·¸ë˜í”„ ìƒì„± ë° ì»´íŒŒì¼
    
    Args:
        checkpointer: ìƒíƒœ ì €ì¥ì„ ìœ„í•œ ì²´í¬í¬ì¸í„° (ì˜µì…˜)
    
    Returns:
        compiled_graph: ì»´íŒŒì¼ëœ LangGraph ê°ì²´
    """
    print("ğŸ”§ ì •ë³´ ìˆ˜ì§‘ ê·¸ë˜í”„ êµ¬ì„± ì¤‘...")
    
    # ê·¸ë˜í”„ ì´ˆê¸°í™”
    workflow = StateGraph(ConversationState)
    
    # ë…¸ë“œ ì¶”ê°€
    
    workflow.add_node("gather_info", information_gathering_node)
    workflow.add_node("next_node", next_node)
    workflow.set_entry_point("gather_info")
    
    # ì¡°ê±´ë¶€ ì—£ì§€: ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ ì—¬ë¶€ì— ë”°ë¥¸ ë¶„ê¸°
    workflow.add_conditional_edges(
        "gather_info",
        should_continue_gathering,
        {
            "complete": "next_node",     # ëª¨ë“  ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ â†’ ì¢…ë£Œ (ë‹¤ìŒ ë…¸ë“œë¡œ ì´ë™)
            "continue": END  # ë” ìˆ˜ì§‘ í•„ìš” â†’ ë‹¤ì‹œ ì •ë³´ ìˆ˜ì§‘
        }
    )

    
    return workflow.compile()


